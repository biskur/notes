# 引言
## 概念
**模式识别**(pattern recognition)领域关注的是通过使用计算机算法自动发现数据中的规律性，并利用这些规律性采取诸如将数据分为不同类别等行动。
通过采用**机器学习**(machine learning)的方法可以得到更好的结果，在这种方法中，大量的$N$个数字集$\mathbf{x}=\{x_1, \cdots, x_N\}$被称为**训练集**(training set)，用于调整自适应模型的参数。训练集中数据的类别事先已知，通常是通过对它们进行单独检查和手工标注，形成**目标向量**(target vector)$\mathbf{t}$。我们可以用目标向量$t$来表示一个数字的类别，它代表了相应数字的身份。
运行机器学习算法的结果可以表示为一个函数$\mathbf{y}(\mathbf{x})$，输出向量的编码方式与目标向量相同。确定函数$\mathbf{y}(\mathbf{x})$的精确形式是在**训练阶段**(training phase)，也叫**学习阶段**(learning phase)。一旦模型经过训练，它就可以确定新数字图像的身份，这些图像被称为构成**测试集**(test set)。对不同于训练所用的新例子进行正确分类的能力被称为**泛化**(generalization)。在实际应用中，输入向量的可变性将使训练数据只能包含所有可能的输入向量中的一小部分，因此泛化是模式识别的一个核心目标。
对于大多数的实际应用来说，原始的输入变量通常会被预处理，以将其转化为一些新的变量空间。预处理阶段有时也被称为**特征提取**(feature extraction)。
训练数据包括输入向量的例子和相应的目标向量的应用被称为**监督学习**(supervised learning)。像数字识别这样的例子，其目的是将每个输入向量分配到有限数量的离散类别中的一个，被称为**分类**(classification)问题。如果所需的输出由一个或多个连续变量组成，那么这个任务就被称为**回归**(regression)问题。回归问题的一个例子是化学生产过程中产量的预估，其中输入包括反应物的浓度、温度和压力。
在其他模式识别问题中，训练数据由一组没有任何对应目标值的输入向量$\mathbf{x}$组成。在这种**无监督学习**(unsupervised learning)问题中，目标可能是在数据中发现类似的样例组，这称为**聚类**(clustering)，或者确定数据在输入空间中的分布，称为**密度估计**(density estimation)，或者为了**可视化**(visualization)的目的，将数据从高维空间投射到二维或三维空间。
与监督学习不同的是，**强化学习**(reinforcement learning)算法并没有得到最佳输出的例子，而是必须通过试错的过程来发现它们。通常情况下，学习算法与环境的互动有一个状态和动作的序列。在许多情况下，当前的动作不仅影响到眼前的奖励，而且还对所有后续时间步长的奖励产生影响。强化学习的一个普遍特征是在**探索**(exploration)和**利用**(exploitation)之间进行权衡，前者是系统尝试新的行动，看看它们的效果如何，后者是系统利用已知的高回报行动。过于注重探索或利用都会产生不良结果。
如果想让机器学习技术在实际应用中发挥最大作用的话，必须要清楚地理解三个重要工具：**概率论**(probability theory)、**决策论**(decision theory)、**信息论**(information theory)。
## 以多项式拟合为例
以多项式拟合为例，现在我们考虑$x$的$N$个观测量组成的向量，记作$\mathbf{x}\equiv(x_1,\cdots,x_N)^T$，和对应的目标变量，记作$\mathbf{t}\equiv(t_1,\cdots,t_N)^T$。我们的目标是利用这个训练集，以便对输入变量的某个新值$\hat{x}$进行目标变量值t的预测。观察到的数据被噪声污染，因此对于给定的$\hat{x}$，合适的$\hat{t}$也是不确定的。概率论提供了一个以精确和定量的方式表达这种不确定性的框架，而决策论允许我们利用这种概率表示，以便根据适当的标准做出最优的预测。
目前我们将并考虑一种基于曲线拟合的简单方法，使用一个多项式函数来拟合数据，其形式为
$$y(x,\mathbf{w})=w_0+w_1x+w_2x^2+\cdots+w_Mx^M=\sum_{j=0}^{M}w_jx^j$$
其中$M$是多项式的**阶数**(order)，$x^j$表示$x$的$j$次幂，多项式的系数$w_0,\cdots,w_M$统称为向量$\mathbf{w}$。注意，多项式函数$y(x,\mathbf{w})$虽然是$x$的非线性函数，但它是系数$\mathbf{w}$的线性函数，未知参数为线性的多项式等函数具有重要的性质，称为**线性模型**(linear model)。
系数的值将通过对训练数据拟合多项式来确定。这可以通过最小化一个**误差函数**(error function)来实现，该误差函数衡量函数$y(x,\mathbf{w})$与训练集数据点之间的拟合度，对于任何给定的$\mathbf{w}$值而言。一个简单而广泛使用的误差函数是由每个数据点$x_n$的预测$y(x_n,\mathbf{w})$和相应的目标值$t_n$之间的误差的平方和给出，因此我们最小化
$$E(\mathbf{w})=\cfrac12\sum_{n=1}^{N}\{y(x_n,\mathbf{w}-t_n)\}^2$$
其中因子$1/2$是为了以后方便处理。
我们可以通过选择$\mathbf{w}$的值，使$E(\mathbf{w})$尽可能小来解决曲线拟合问题。由于误差函数是系数$\mathbf{w}$的二次函数，其相对于系数的导数将在$\mathbf{w}$的元素中呈线性，因此误差函数的最小化有一个唯一解，用$\mathbf{w}^\star$表示，可以用封闭形式找到。所得多项式由函数$y(x,\mathbf{w}^\star)$给出。
剩下的问题是如何选择多项式的阶数，这是一个被称为**模型对比**(model comparison)或**模型选择**(model selection)的重要概念的一个例子。
常数($M=0$)和一阶($M=1$)多项式对数据的拟合度很差，三阶($M=3$)多项式似乎对函数的拟合度最好。当采用更高阶的多项式($M=9$)时，获得了对训练数据的极好拟合。事实上，多项式精确地通过每个数据点，并且$E(\mathbf{w}^\star)=0$。然而，拟合的曲线疯狂地摆动，并给出了一个非常糟糕的函数$\sin(2πx)$表示。这被称为**过度拟合**(over-fitting)。
测试集误差衡量我们在预测$x$的新数据观测值$t$的数值方面做得如何。
对于给定的模型复杂性，随着数据集规模的增加，过度拟合问题变得不那么严重。另一种说法是，数据集越大，我们能承受的数据拟合模型越复杂，换句话说就是越灵活。有时提倡的一个粗略的启发式方法是，数据点的数量不应少于模型中自适应参数数量的某个倍数，比如5或10。
另外，根据可用训练集的大小来限制模型中的参数数量，这一点也是相当令人不满意的。根据所要解决的问题的复杂程度来选择模型的复杂程度似乎更为合理。我们将看到，最小二乘法寻找模型参数的方法代表了**最大似然**(maximum likelihood)的一种特殊情况，而过拟合问题可以理解为最大似然的一般属性。通过采用**贝叶斯**(Bayesian)方法，可以避免过拟合问题。我们将看到，从贝叶斯的角度来看，采用参数数量大大超过数据点数量的模型并不困难。在贝叶斯模型中，参数的**有效**(effective)数量会自动适应数据集的大小。
经常用来控制过拟合现象的一种技术是**正则化**(regularization)技术，即在误差函数中加入一个惩罚项，以阻止系数达到大值。最简单的惩罚项的形式是所有系数的平方和，因而修改后的误差函数的形式为
$$\tilde{E}(\mathbf{w})=\cfrac12\sum_{n=1}^N\{y(x_n,\mathbf{w}-t_n)\}^2+\cfrac{\lambda}{2}\|\mathbf{w}\|^2$$
其中，$\|\mathbf{w}\|^2\equiv\mathbf{w}^T\mathbf{w}=w_0^2+w_1^2+\cdots+w_M^2$，系数$λ$控制着正则化项与二乘法误差项的相对重要性。需要注意的是，正则化器中通常会省略系数$w_0$，因为它的加入会导致结果取决于目标变量的原点选择，或者它可能被包括在内，但有自己的正则化系数。同样，误差函数可以完全以闭合形式最小化。像这样的技术在统计学文献中被称为**收缩**(shrinkage)法，因为它们减少了系数的值。平方正则化器的特殊情况被称为**脊回归**(ridge regression)。在神经网络的背景下，这种方法被称为**权衰减**(weight decay)。
模型复杂度的问题是一个重要的问题，如果我们试图用这种最小化误差函数的方法来解决实际应用，我们就必须找到一种方法来确定模型复杂度的合适值。上面的结果提出了一个简单的实现方法，即把现有的数据分割成一个训练集，用来确定系数$\mathbf{w}$，和一个单独的验证集，也称为**保留**(hold-out)集，用来优化模型复杂度($M$或$λ$)。然而，在很多情况下，这将被证明太浪费宝贵的训练数据，必须寻求更复杂的方法。
